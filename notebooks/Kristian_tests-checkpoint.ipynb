{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; Pkg.activate(\"./\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/project_TE/package_CMI_prague/Project.toml`\n",
      " \u001b[90m [5732040d] \u001b[39m\u001b[37mDelayEmbeddings v1.20.0\u001b[39m\n",
      " \u001b[90m [634d3b9d] \u001b[39m\u001b[37mDrWatson v1.16.6\u001b[39m\n",
      " \u001b[90m [ed8fcbec] \u001b[39m\u001b[37mEntropies v0.1.0\u001b[39m\n",
      " \u001b[90m [fd094767] \u001b[39m\u001b[37mSuppressor v0.2.0\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.status()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pkg.add(\"DelayEmbeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pkg.add(\"Entropies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelayEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?SymbolicPermutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?Entropies.genentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(111)\n",
    "y = rand(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "genembed(s, τs, js = ones(...); ws = nothing) → dataset\n",
       "\\end{verbatim}\n",
       "Create a generalized embedding of \\texttt{s} which can be a timeseries or arbitrary \\texttt{Dataset}, and return the result as a new \\texttt{Dataset}.\n",
       "\n",
       "The generalized embedding works as follows:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{τs} denotes what delay times will be used for each of the entries of the delay vector. It is recommended that \\texttt{τs[1] = 0}. \\texttt{τs} is allowed to have \\emph{negative entries} as well.\n",
       "\n",
       "\n",
       "\\item \\texttt{js} denotes which of the timeseries contained in \\texttt{s} will be used for the entries of the delay vector. \\texttt{js} can contain duplicate indices.\n",
       "\n",
       "\n",
       "\\item \\texttt{ws} are optional weights that weight each embedded entry (the i-th entry of the   delay vector is weighted by \\texttt{ws[i]}). If provided, it is recommended that \\texttt{ws[1] = 1}\n",
       "\n",
       "\\end{itemize}\n",
       "\\texttt{τs, js, ws} are tuples (or vectors) of length \\texttt{D}, which also coincides with the embedding dimension. For example, imagine input trajectory $s = [x, y, z]$ where $x, y, z$ are timeseries (the columns of the \\texttt{Dataset}). If \\texttt{js = (1, 3, 2)} and \\texttt{τs = (0, 2, -7)} the created delay vector at each step $n$ will be\n",
       "\n",
       "$$(x(n), z(n+2), y(n-7))$$\n",
       "Using \\texttt{ws = (1, 0.5, 0.25)} as well would create\n",
       "\n",
       "$$(x(n), \\frac{1}{2} z(n+2), \\frac{1}{4} y(n-7))$$\n",
       "\\texttt{js} can be skipped, defaulting to index 1 (first timeseries) for all delay entries, while it has no effect if \\texttt{s} is a timeseries instead of a \\texttt{Dataset}.\n",
       "\n",
       "See also \\href{@ref}{\\texttt{embed}}. Internally uses \\href{@ref}{\\texttt{GeneralizedEmbedding}}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "genembed(s, τs, js = ones(...); ws = nothing) → dataset\n",
       "```\n",
       "\n",
       "Create a generalized embedding of `s` which can be a timeseries or arbitrary `Dataset`, and return the result as a new `Dataset`.\n",
       "\n",
       "The generalized embedding works as follows:\n",
       "\n",
       "  * `τs` denotes what delay times will be used for each of the entries of the delay vector. It is recommended that `τs[1] = 0`. `τs` is allowed to have *negative entries* as well.\n",
       "  * `js` denotes which of the timeseries contained in `s` will be used for the entries of the delay vector. `js` can contain duplicate indices.\n",
       "  * `ws` are optional weights that weight each embedded entry (the i-th entry of the   delay vector is weighted by `ws[i]`). If provided, it is recommended that `ws[1] = 1`\n",
       "\n",
       "`τs, js, ws` are tuples (or vectors) of length `D`, which also coincides with the embedding dimension. For example, imagine input trajectory $s = [x, y, z]$ where $x, y, z$ are timeseries (the columns of the `Dataset`). If `js = (1, 3, 2)` and `τs = (0, 2, -7)` the created delay vector at each step $n$ will be\n",
       "\n",
       "$$\n",
       "(x(n), z(n+2), y(n-7))\n",
       "$$\n",
       "\n",
       "Using `ws = (1, 0.5, 0.25)` as well would create\n",
       "\n",
       "$$\n",
       "(x(n), \\frac{1}{2} z(n+2), \\frac{1}{4} y(n-7))\n",
       "$$\n",
       "\n",
       "`js` can be skipped, defaulting to index 1 (first timeseries) for all delay entries, while it has no effect if `s` is a timeseries instead of a `Dataset`.\n",
       "\n",
       "See also [`embed`](@ref). Internally uses [`GeneralizedEmbedding`](@ref).\n"
      ],
      "text/plain": [
       "\u001b[36m  genembed(s, τs, js = ones(...); ws = nothing) → dataset\u001b[39m\n",
       "\n",
       "  Create a generalized embedding of \u001b[36ms\u001b[39m which can be a timeseries or arbitrary\n",
       "  \u001b[36mDataset\u001b[39m, and return the result as a new \u001b[36mDataset\u001b[39m.\n",
       "\n",
       "  The generalized embedding works as follows:\n",
       "\n",
       "    •    \u001b[36mτs\u001b[39m denotes what delay times will be used for each of the entries\n",
       "        of the delay vector. It is recommended that \u001b[36mτs[1] = 0\u001b[39m. \u001b[36mτs\u001b[39m is\n",
       "        allowed to have \u001b[4mnegative entries\u001b[24m as well.\n",
       "\n",
       "    •    \u001b[36mjs\u001b[39m denotes which of the timeseries contained in \u001b[36ms\u001b[39m will be used for\n",
       "        the entries of the delay vector. \u001b[36mjs\u001b[39m can contain duplicate indices.\n",
       "\n",
       "    •    \u001b[36mws\u001b[39m are optional weights that weight each embedded entry (the i-th\n",
       "        entry of the delay vector is weighted by \u001b[36mws[i]\u001b[39m). If provided, it\n",
       "        is recommended that \u001b[36mws[1] = 1\u001b[39m\n",
       "\n",
       "  \u001b[36mτs, js, ws\u001b[39m are tuples (or vectors) of length \u001b[36mD\u001b[39m, which also coincides with\n",
       "  the embedding dimension. For example, imagine input trajectory \u001b[35ms = [x, y, z]\u001b[39m\n",
       "  where \u001b[35mx, y, z\u001b[39m are timeseries (the columns of the \u001b[36mDataset\u001b[39m). If \u001b[36mjs = (1, 3, 2)\u001b[39m\n",
       "  and \u001b[36mτs = (0, 2, -7)\u001b[39m the created delay vector at each step \u001b[35mn\u001b[39m will be\n",
       "\n",
       "\u001b[35m  (x(n), z(n+2), y(n-7))\u001b[39m\n",
       "\n",
       "  Using \u001b[36mws = (1, 0.5, 0.25)\u001b[39m as well would create\n",
       "\n",
       "\u001b[35m  (x(n), \\frac{1}{2} z(n+2), \\frac{1}{4} y(n-7))\u001b[39m\n",
       "\n",
       "  \u001b[36mjs\u001b[39m can be skipped, defaulting to index 1 (first timeseries) for all delay\n",
       "  entries, while it has no effect if \u001b[36ms\u001b[39m is a timeseries instead of a \u001b[36mDataset\u001b[39m.\n",
       "\n",
       "  See also \u001b[36membed\u001b[39m. Internally uses \u001b[36mGeneralizedEmbedding\u001b[39m."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DelayEmbeddings.genembed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "TE_{x \\to y} = \\sum p(x_t, y_t, y_{t+\\eta}) \\log \\dfrac{p(y_{t+\\eta} | y_t, x_t)}{p(y_{t+\\eta} | y_t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-dimensional Dataset{Float64} with 110 points\n",
       " 0.742872    0.841675   0.484482\n",
       " 0.922236    0.484482   0.502265\n",
       " 0.00472441  0.502265   0.665298\n",
       " 0.438385    0.665298   0.736781\n",
       " 0.0923076   0.736781   0.688567\n",
       " 0.751574    0.688567   0.191833\n",
       " 0.00545923  0.191833   0.887844\n",
       " 0.891561    0.887844   0.513661\n",
       " 0.518826    0.513661   0.101896\n",
       " 0.310884    0.101896   0.364202\n",
       " 0.923854    0.364202   0.0622038\n",
       " 0.0880226   0.0622038  0.23342\n",
       " 0.717915    0.23342    0.736318\n",
       " ⋮                      \n",
       " 0.948769    0.273191   0.0884753\n",
       " 0.911863    0.0884753  0.77557\n",
       " 0.23425     0.77557    0.886121\n",
       " 0.368427    0.886121   0.487611\n",
       " 0.858657    0.487611   0.498866\n",
       " 0.670798    0.498866   0.791045\n",
       " 0.660356    0.791045   0.588285\n",
       " 0.842609    0.588285   0.873598\n",
       " 0.318514    0.873598   0.0166574\n",
       " 0.771333    0.0166574  0.859447\n",
       " 0.479545    0.859447   0.911151\n",
       " 0.9339      0.911151   0.981272"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint= DelayEmbeddings.genembed(Dataset(x, y),  (0,0,1), (1,2,2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "genentropy(p::Probabilities; q = 1.0, base = MathConstants.e)\n",
       "\\end{verbatim}\n",
       "Compute the generalized order-\\texttt{q} entropy of some probabilities returned by the \\href{@ref}{\\texttt{probabilities}} function. Alternatively, compute entropy from pre-computed \\texttt{Probabilities}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "genentropy(x::Vector_or_Dataset, est; q = 1.0, base)\n",
       "\\end{verbatim}\n",
       "A convenience syntax, which calls first \\texttt{probabilities(x, est)} and then calculates the entropy of the result (and thus \\texttt{est} can be a \\texttt{ProbabilitiesEstimator} or simply \\texttt{ε::Real}).\n",
       "\n",
       "\\subsection{Description}\n",
       "Let $p$ be an array of probabilities (summing to 1). Then the generalized (Rényi) entropy is\n",
       "\n",
       "$$H_q(p) = \\frac{1}{1-q} \\log \\left(\\sum_i p[i]^q\\right)$$\n",
       "and generalizes other known entropies, like e.g. the information entropy ($q = 1$, see \\footnotemark[Shannon1948]), the maximum entropy ($q=0$, also known as Hartley entropy), or the correlation entropy ($q = 2$, also known as collision entropy).\n",
       "\n",
       "\\footnotetext[Rényi1960]{A. Rényi, \\emph{Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability}, pp 547 (1960)\n",
       "\n",
       "}\n",
       "\\footnotetext[Shannon1948]{C. E. Shannon, Bell Systems Technical Journal \\textbf{27}, pp 379 (1948)\n",
       "\n",
       "}\n"
      ],
      "text/markdown": [
       "```\n",
       "genentropy(p::Probabilities; q = 1.0, base = MathConstants.e)\n",
       "```\n",
       "\n",
       "Compute the generalized order-`q` entropy of some probabilities returned by the [`probabilities`](@ref) function. Alternatively, compute entropy from pre-computed `Probabilities`.\n",
       "\n",
       "```\n",
       "genentropy(x::Vector_or_Dataset, est; q = 1.0, base)\n",
       "```\n",
       "\n",
       "A convenience syntax, which calls first `probabilities(x, est)` and then calculates the entropy of the result (and thus `est` can be a `ProbabilitiesEstimator` or simply `ε::Real`).\n",
       "\n",
       "## Description\n",
       "\n",
       "Let $p$ be an array of probabilities (summing to 1). Then the generalized (Rényi) entropy is\n",
       "\n",
       "$$\n",
       "H_q(p) = \\frac{1}{1-q} \\log \\left(\\sum_i p[i]^q\\right)\n",
       "$$\n",
       "\n",
       "and generalizes other known entropies, like e.g. the information entropy ($q = 1$, see [^Shannon1948]), the maximum entropy ($q=0$, also known as Hartley entropy), or the correlation entropy ($q = 2$, also known as collision entropy).\n",
       "\n",
       "[^Rényi1960]: A. Rényi, *Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability*, pp 547 (1960)\n",
       "\n",
       "[^Shannon1948]: C. E. Shannon, Bell Systems Technical Journal **27**, pp 379 (1948)\n"
      ],
      "text/plain": [
       "\u001b[36m  genentropy(p::Probabilities; q = 1.0, base = MathConstants.e)\u001b[39m\n",
       "\n",
       "  Compute the generalized order-\u001b[36mq\u001b[39m entropy of some probabilities returned by\n",
       "  the \u001b[36mprobabilities\u001b[39m function. Alternatively, compute entropy from pre-computed\n",
       "  \u001b[36mProbabilities\u001b[39m.\n",
       "\n",
       "\u001b[36m  genentropy(x::Vector_or_Dataset, est; q = 1.0, base)\u001b[39m\n",
       "\n",
       "  A convenience syntax, which calls first \u001b[36mprobabilities(x, est)\u001b[39m and then\n",
       "  calculates the entropy of the result (and thus \u001b[36mest\u001b[39m can be a\n",
       "  \u001b[36mProbabilitiesEstimator\u001b[39m or simply \u001b[36mε::Real\u001b[39m).\n",
       "\n",
       "\u001b[1m  Description\u001b[22m\n",
       "\u001b[1m  =============\u001b[22m\n",
       "\n",
       "  Let \u001b[35mp\u001b[39m be an array of probabilities (summing to 1). Then the generalized\n",
       "  (Rényi) entropy is\n",
       "\n",
       "\u001b[35m  H_q(p) = \\frac{1}{1-q} \\log \\left(\\sum_i p[i]^q\\right)\u001b[39m\n",
       "\n",
       "  and generalizes other known entropies, like e.g. the information entropy (\u001b[35mq\n",
       "  = 1\u001b[39m, see \u001b[1m[^Shannon1948]\u001b[22m), the maximum entropy (\u001b[35mq=0\u001b[39m, also known as Hartley\n",
       "  entropy), or the correlation entropy (\u001b[35mq = 2\u001b[39m, also known as collision\n",
       "  entropy).\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Rényi1960]\u001b[22m\n",
       "  │\n",
       "  │  A. Rényi, \u001b[4mProceedings of the fourth Berkeley Symposium on\n",
       "  │  Mathematics, Statistics and Probability\u001b[24m, pp 547 (1960)\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Shannon1948]\u001b[22m\n",
       "  │\n",
       "  │  C. E. Shannon, Bell Systems Technical Journal \u001b[1m27\u001b[22m, pp 379 (1948)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Entropies.genentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "TE_{x \\to y} = - H(x_y, y_t, y_{t+\\eta}) + H(y_{t+\\eta}, y_t) +  H(x_y, y_t)  - H(y_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "\u001b[91mUndefVarError: KozachenkoLeonenko not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: KozachenkoLeonenko not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[8]:2"
     ]
    }
   ],
   "source": [
    "b = Entropies.RectangularBinning(50)\n",
    "H3 = Entropies.genentropy(joint, KozachenkoLeonenko(1,10))# α =1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "\u001b[91mMethodError: no method matching genentropy(::Dataset{2,Float64}, ::VisitationFrequency, ::Int64; α=1.0)\u001b[39m\n\u001b[91m\u001b[0mClosest candidates are:\u001b[39m\n\u001b[91m\u001b[0m  genentropy(::AbstractDataset, ::VisitationFrequency, ::Real; base) at /home/arinyoprats/.julia/packages/Entropies/SLwaH/src/binning_based/rectangular/VisitationFrequency.jl:126\u001b[91m got unsupported keyword argument \"α\"\u001b[39m\u001b[39m\n\u001b[91m\u001b[0m  genentropy(::AbstractDataset{m,T}, \u001b[91m::CountOccurrences\u001b[39m, ::Real; base) where {m, T<:Real} at /home/arinyoprats/.julia/packages/Entropies/SLwaH/src/counting_based/CountOccurrences.jl:52\u001b[91m got unsupported keyword argument \"α\"\u001b[39m\u001b[39m\n\u001b[91m\u001b[0m  genentropy(::AbstractDataset{N,T}, \u001b[91m::SymbolicPermutation\u001b[39m, ::Real; base) where {N, T} at /home/arinyoprats/.julia/packages/Entropies/SLwaH/src/symbolic/SymbolicPermutation.jl:288\u001b[91m got unsupported keyword argument \"α\"\u001b[39m\u001b[39m\n\u001b[91m\u001b[0m  ...\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethodError: no method matching genentropy(::Dataset{2,Float64}, ::VisitationFrequency, ::Int64; α=1.0)\u001b[39m\n\u001b[91m\u001b[0mClosest candidates are:\u001b[39m\n\u001b[91m\u001b[0m  genentropy(::AbstractDataset, ::VisitationFrequency, ::Real; base) at /home/arinyoprats/.julia/packages/Entropies/SLwaH/src/binning_based/rectangular/VisitationFrequency.jl:126\u001b[91m got unsupported keyword argument \"α\"\u001b[39m\u001b[39m\n\u001b[91m\u001b[0m  genentropy(::AbstractDataset{m,T}, \u001b[91m::CountOccurrences\u001b[39m, ::Real; base) where {m, T<:Real} at /home/arinyoprats/.julia/packages/Entropies/SLwaH/src/counting_based/CountOccurrences.jl:52\u001b[91m got unsupported keyword argument \"α\"\u001b[39m\u001b[39m\n\u001b[91m\u001b[0m  genentropy(::AbstractDataset{N,T}, \u001b[91m::SymbolicPermutation\u001b[39m, ::Real; base) where {N, T} at /home/arinyoprats/.julia/packages/Entropies/SLwaH/src/symbolic/SymbolicPermutation.jl:288\u001b[91m got unsupported keyword argument \"α\"\u001b[39m\u001b[39m\n\u001b[91m\u001b[0m  ...\u001b[39m",
      "",
      "Stacktrace:",
      " [1] kwerr(::NamedTuple{(:α,),Tuple{Float64}}, ::Function, ::Dataset{2,Float64}, ::VisitationFrequency, ::Int64) at ./error.jl:157",
      " [2] top-level scope at In[9]:1"
     ]
    }
   ],
   "source": [
    "H2a =Entropies.genentropy(joint[:,[3,2]], VisitationFrequency(b), α =1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.675275013772055"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2b=Entropies.genentropy(joint[:,[1,2]], VisitationFrequency(b), α =1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6766536997888504"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H1=Entropies.genentropy(joint[:,[2]], VisitationFrequency(b), α =1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9986213139832043"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-H3+H2a+H2b-H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mV\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mF\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mq\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1my\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "VisitationFrequency(r::RectangularBinning) <: BinningProbabilitiesEstimator\n",
       "\\end{verbatim}\n",
       "A probability estimator based on binning data into rectangular boxes dictated by the binning scheme \\texttt{r}.\n",
       "\n",
       "\\subsection{Example}\n",
       "\\begin{verbatim}\n",
       "# Construct boxes by dividing each coordinate axis into 5 equal-length chunks.\n",
       "b = RectangularBinning(5)\n",
       "\n",
       "# A probabilities estimator that, when applied a dataset, computes visitation frequencies\n",
       "# over the boxes of the binning, constructed as describedon the previous line.\n",
       "est = VisitationFrequency(b)\n",
       "\\end{verbatim}\n",
       "See also: \\href{@ref}{\\texttt{RectangularBinning}}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "VisitationFrequency(r::RectangularBinning) <: BinningProbabilitiesEstimator\n",
       "```\n",
       "\n",
       "A probability estimator based on binning data into rectangular boxes dictated by the binning scheme `r`.\n",
       "\n",
       "## Example\n",
       "\n",
       "```julia\n",
       "# Construct boxes by dividing each coordinate axis into 5 equal-length chunks.\n",
       "b = RectangularBinning(5)\n",
       "\n",
       "# A probabilities estimator that, when applied a dataset, computes visitation frequencies\n",
       "# over the boxes of the binning, constructed as describedon the previous line.\n",
       "est = VisitationFrequency(b)\n",
       "```\n",
       "\n",
       "See also: [`RectangularBinning`](@ref).\n"
      ],
      "text/plain": [
       "\u001b[36m  VisitationFrequency(r::RectangularBinning) <: BinningProbabilitiesEstimator\u001b[39m\n",
       "\n",
       "  A probability estimator based on binning data into rectangular boxes\n",
       "  dictated by the binning scheme \u001b[36mr\u001b[39m.\n",
       "\n",
       "\u001b[1m  Example\u001b[22m\n",
       "\u001b[1m  =========\u001b[22m\n",
       "\n",
       "\u001b[36m  # Construct boxes by dividing each coordinate axis into 5 equal-length chunks.\u001b[39m\n",
       "\u001b[36m  b = RectangularBinning(5)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # A probabilities estimator that, when applied a dataset, computes visitation frequencies\u001b[39m\n",
       "\u001b[36m  # over the boxes of the binning, constructed as describedon the previous line.\u001b[39m\n",
       "\u001b[36m  est = VisitationFrequency(b)\u001b[39m\n",
       "\n",
       "  See also: \u001b[36mRectangularBinning\u001b[39m."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?VisitationFrequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6655206093119226"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = RectangularBinning(6)\n",
    "H3 = Entropies.genentropy(joint, VisitationFrequency(b), α =1.0)\n",
    "H2a =Entropies.genentropy(joint[:,[3,2]], VisitationFrequency(b), α =1.0)\n",
    "H2b=Entropies.genentropy(joint[:,[1,2]], VisitationFrequency(b), α =1.0)\n",
    "H1=Entropies.genentropy(joint[:,[2]], VisitationFrequency(b), α =1.0)\n",
    "T =-H3+H2a+H2b-H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = RectangularBinning(6)\n",
    "H3 = Entropies.genentropy(joint, VisitationFrequency(b), α =1.0)\n",
    "H2a =Entropies.genentropy(joint[:,[3,2]], VisitationFrequency(b), α =1.0)\n",
    "H2b=Entropies.genentropy(joint[:,[1,2]], VisitationFrequency(b), α =1.0)\n",
    "H1=Entropies.genentropy(joint[:,[2]], VisitationFrequency(b), α =1.0)\n",
    "T =-H3+H2a+H2b-H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\subsection{k-th nearest neighbour(kNN) based}\n",
       "\\begin{verbatim}\n",
       "Kraskov(k::Int = 1, w::Int = 1) <: NearestNeighborEntropyEstimator\n",
       "\\end{verbatim}\n",
       "Entropy estimator based on \\texttt{k}-th nearest neighbor searches\\footnotemark[Kraskov2004].\n",
       "\n",
       "\\texttt{w} is the number of nearest neighbors to exclude when searching for neighbours  (defaults to \\texttt{0}, meaning that only the point itself is excluded).\n",
       "\n",
       "\\begin{quote}\n",
       "\\textbf{info}\n",
       "\n",
       "Info\n",
       "\n",
       "This estimator is only available for entropy estimation. Probabilities  cannot be obtained directly.\n",
       "\n",
       "\\end{quote}\n",
       "\\footnotetext[Kraskov2004]{Kraskov, A., Stögbauer, H., \\& Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n",
       "\n",
       "}\n"
      ],
      "text/markdown": [
       "## k-th nearest neighbour(kNN) based\n",
       "\n",
       "```\n",
       "Kraskov(k::Int = 1, w::Int = 1) <: NearestNeighborEntropyEstimator\n",
       "```\n",
       "\n",
       "Entropy estimator based on `k`-th nearest neighbor searches[^Kraskov2004].\n",
       "\n",
       "`w` is the number of nearest neighbors to exclude when searching for neighbours  (defaults to `0`, meaning that only the point itself is excluded).\n",
       "\n",
       "!!! info\n",
       "    This estimator is only available for entropy estimation. Probabilities  cannot be obtained directly.\n",
       "\n",
       "\n",
       "[^Kraskov2004]: Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n"
      ],
      "text/plain": [
       "\u001b[1m  k-th nearest neighbour(kNN) based\u001b[22m\n",
       "\u001b[1m  ===================================\u001b[22m\n",
       "\n",
       "\u001b[36m  Kraskov(k::Int = 1, w::Int = 1) <: NearestNeighborEntropyEstimator\u001b[39m\n",
       "\n",
       "  Entropy estimator based on \u001b[36mk\u001b[39m-th nearest neighbor searches\u001b[1m[^Kraskov2004]\u001b[22m.\n",
       "\n",
       "  \u001b[36mw\u001b[39m is the number of nearest neighbors to exclude when searching for\n",
       "  neighbours (defaults to \u001b[36m0\u001b[39m, meaning that only the point itself is excluded).\n",
       "\n",
       "\u001b[36m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo\u001b[22m\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  This estimator is only available for entropy estimation.\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  Probabilities cannot be obtained directly.\n",
       "\n",
       "  │ \u001b[0m\u001b[1m[^Kraskov2004]\u001b[22m\n",
       "  │\n",
       "  │  Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating\n",
       "  │  mutual information. Physical review E, 69(6), 066138."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Entropies.Kraskov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching genentropy(::Dataset{3,Float64}, ::Kraskov; α=1.0)\nClosest candidates are:\n  genentropy(::Dataset{D,T}, ::Kraskov; base) where {D, T} at /Users/andreu/.julia/packages/Entropies/Y329Q/src/nearest_neighbors/Kraskov.jl:29 got unsupported keyword argument \"α\"\n  genentropy(::Union{AbstractArray{T,1} where T, Dataset}, ::Any; α, base) at /Users/andreu/.julia/packages/Entropies/Y329Q/src/core.jl:138\n  genentropy(::Dataset{D,T}, !Matched::KozachenkoLeonenko; base) where {D, T} at /Users/andreu/.julia/packages/Entropies/Y329Q/src/nearest_neighbors/KozachenkoLeonenko.jl:31 got unsupported keyword argument \"α\"",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching genentropy(::Dataset{3,Float64}, ::Kraskov; α=1.0)\nClosest candidates are:\n  genentropy(::Dataset{D,T}, ::Kraskov; base) where {D, T} at /Users/andreu/.julia/packages/Entropies/Y329Q/src/nearest_neighbors/Kraskov.jl:29 got unsupported keyword argument \"α\"\n  genentropy(::Union{AbstractArray{T,1} where T, Dataset}, ::Any; α, base) at /Users/andreu/.julia/packages/Entropies/Y329Q/src/core.jl:138\n  genentropy(::Dataset{D,T}, !Matched::KozachenkoLeonenko; base) where {D, T} at /Users/andreu/.julia/packages/Entropies/Y329Q/src/nearest_neighbors/KozachenkoLeonenko.jl:31 got unsupported keyword argument \"α\"",
      "",
      "Stacktrace:",
      " [1] kwerr(::NamedTuple{(:α,),Tuple{Float64}}, ::Function, ::Dataset{3,Float64}, ::Kraskov) at ./error.jl:157",
      " [2] top-level scope at In[39]:2",
      " [3] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "b =2\n",
    "H3 = Entropies.genentropy(joint, Kraskov(k=b), α =1.0)\n",
    "H2a =Entropies.genentropy(joint[:,[3,2]], Kraskov(k=b), α =1.0)\n",
    "H2b=Entropies.genentropy(joint[:,[1,2]], Kraskov(k=b), α =1.0)\n",
    "H1=Entropies.genentropy(joint[:,[2]], Kraskov(k=b), α =1.0)\n",
    "T =-H3+H2a+H2b-H1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
